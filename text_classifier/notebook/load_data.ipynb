{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a09efe0",
   "metadata": {},
   "source": [
    "# Multi-Label Text Classification - Data Preparation\n",
    "\n",
    "This notebook prepares the Dell Research Harvard newswire dataset for **multi-label text classification** with **one-hot encoded labels**.\n",
    "\n",
    "**Key Points:**\n",
    "- 7 binary classification tasks (one per category)\n",
    "- Labels are stored as 7-dimensional binary vectors\n",
    "- Each dimension represents: antitrust, civil_rights, crime, govt_regulation, labor_movement, politics, protests\n",
    "- Example: `[0, 0, 1, 1, 0, 1, 0]` means the article is about crime, govt_regulation, and politics\n",
    "\n",
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0whxry4vfb6n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from HuggingFace...\n",
      "\n",
      "✓ Dataset loaded successfully!\n",
      "  Total articles: 65,191\n",
      "  Total features: 23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Dell Research Harvard newswire dataset\n",
    "print(\"Loading dataset from HuggingFace...\")\n",
    "dataset = load_dataset(\n",
    "    \"dell-research-harvard/newswire\",\n",
    "    data_files=[\"1968_data_clean.json\", \"1969_data_clean.json\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded successfully!\")\n",
    "print(f\"  Total articles: {dataset['train'].num_rows:,}\")\n",
    "\n",
    "# Convert to pandas\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "print(f\"  Total features: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7u4w69dobr9",
   "metadata": {},
   "source": [
    "## 2. Extract Labels and Text\n",
    "\n",
    "Extract the 7 category columns and convert them directly to one-hot encoded arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91418f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category columns:\n",
      "======================================================================\n",
      "  [0] antitrust                93 positive samples (  0.1%)\n",
      "  [1] civil_rights          3,393 positive samples (  5.2%)\n",
      "  [2] crime                 7,464 positive samples ( 11.4%)\n",
      "  [3] govt_regulation       5,590 positive samples (  8.6%)\n",
      "  [4] labor_movement        4,533 positive samples (  7.0%)\n",
      "  [5] politics             24,005 positive samples ( 36.8%)\n",
      "  [6] protests              1,833 positive samples (  2.8%)\n",
      "\n",
      "✓ 7 categories for multi-label classification\n"
     ]
    }
   ],
   "source": [
    "# Define the 7 categories in order\n",
    "CATEGORIES = [\n",
    "    'antitrust',\n",
    "    'civil_rights',\n",
    "    'crime',\n",
    "    'govt_regulation',\n",
    "    'labor_movement',\n",
    "    'politics',\n",
    "    'protests'\n",
    "]\n",
    "\n",
    "print(\"Category columns:\")\n",
    "print(\"=\" * 70)\n",
    "for i, category in enumerate(CATEGORIES):\n",
    "    count = df[category].sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  [{i}] {category:20s} {count:6,} positive samples ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n✓ 7 categories for multi-label classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwz1uk8z8zn",
   "metadata": {},
   "source": [
    "## 3. Convert to One-Hot Encoded Labels\n",
    "\n",
    "Each article's labels are represented as a 7-dimensional binary vector where 1 indicates the category is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e5b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data extracted successfully!\n",
      "  Texts shape: (65191,)\n",
      "  Labels shape: (65191, 7)\n",
      "  Labels dtype: float32\n",
      "\n",
      "Label Statistics:\n",
      "  Min labels per sample: 0\n",
      "  Max labels per sample: 5\n",
      "  Avg labels per sample: 0.72\n",
      "\n",
      "Distribution by number of labels:\n",
      "  0 label(s): 29,339 samples ( 45.0%)\n",
      "  1 label(s): 26,585 samples ( 40.8%)\n",
      "  2 label(s):  7,596 samples ( 11.7%)\n",
      "  3 label(s):  1,551 samples (  2.4%)\n",
      "  4 label(s):    119 samples (  0.2%)\n",
      "  5 label(s):      1 samples (  0.0%)\n",
      "\n",
      "Sample Data (first 5):\n",
      "======================================================================\n",
      "\n",
      "1. Labels: [0. 0. 0. 0. 0. 0. 0.] → ['no_class']\n",
      "   Text: SAIGON (AP) — Smashing Communist thrusts across South Vietnam, allied forces kil...\n",
      "\n",
      "2. Labels: [0. 0. 1. 0. 1. 0. 0.] → ['crime', 'labor_movement']\n",
      "   Text: NEW YORK (AP) — Police cleared a barricaded building and arrested 131 demonstrat...\n",
      "\n",
      "3. Labels: [0. 0. 0. 0. 0. 0. 0.] → ['no_class']\n",
      "   Text: SAIGON (AP) — The allies called off the Tet truce in South Vietnam’s northern mi...\n",
      "\n",
      "4. Labels: [0. 0. 0. 0. 0. 0. 0.] → ['no_class']\n",
      "   Text: SAIGON (AP) — American warplanes and warships, including B-52 bombers and the ba...\n",
      "\n",
      "5. Labels: [0. 0. 0. 0. 0. 1. 0.] → ['politics']\n",
      "   Text: PARIS — American and North Vietnamese peace envoys traded charges today of aggre...\n"
     ]
    }
   ],
   "source": [
    "# Extract text\n",
    "texts = df['cleaned_article'].values\n",
    "\n",
    "# Extract labels as one-hot encoded numpy arrays\n",
    "# Stack the 7 binary columns into a (n_samples, 7) array\n",
    "labels = df[CATEGORIES].values.astype(np.float32)\n",
    "\n",
    "print(f\"✓ Data extracted successfully!\")\n",
    "print(f\"  Texts shape: {texts.shape}\")\n",
    "print(f\"  Labels shape: {labels.shape}\")\n",
    "print(f\"  Labels dtype: {labels.dtype}\")\n",
    "\n",
    "# Analyze label statistics\n",
    "labels_per_sample = labels.sum(axis=1)\n",
    "print(f\"\\nLabel Statistics:\")\n",
    "print(f\"  Min labels per sample: {int(labels_per_sample.min())}\")\n",
    "print(f\"  Max labels per sample: {int(labels_per_sample.max())}\")\n",
    "print(f\"  Avg labels per sample: {labels_per_sample.mean():.2f}\")\n",
    "\n",
    "# Distribution by number of active labels\n",
    "print(f\"\\nDistribution by number of labels:\")\n",
    "unique_counts, counts = np.unique(labels_per_sample, return_counts=True)\n",
    "for n_labels, count in zip(unique_counts, counts):\n",
    "    pct = (count / len(labels)) * 100\n",
    "    print(f\"  {int(n_labels)} label(s): {count:6,} samples ({pct:5.1f}%)\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nSample Data (first 5):\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(5, len(texts))):\n",
    "    active_categories = [CATEGORIES[j] for j in range(7) if labels[i, j] == 1]\n",
    "    if not active_categories:\n",
    "        active_categories = ['no_class']\n",
    "    text_preview = texts[i][:80] + \"...\" if len(texts[i]) > 80 else texts[i]\n",
    "    print(f\"\\n{i+1}. Labels: {labels[i]} → {active_categories}\")\n",
    "    print(f\"   Text: {text_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ut8qv4yoo",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split\n",
    "\n",
    "Split the data into training (80%) and test (20%) sets.\n",
    "\n",
    "**Note:** Validation data will be automatically created during training by randomly splitting 20% from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39707113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data split complete!\n",
      "\n",
      "  Training set:   52,152 samples (80.0%)\n",
      "  Test set:       13,039 samples (20.0%)\n",
      "  ──────────────────────────────────────────────────\n",
      "  Total:          65,191 samples\n",
      "\n",
      "  Note: Validation set (20% of train) will be created during training\n",
      "\n",
      "Training Set - Labels per sample:\n",
      "  0 label(s): 23,475 samples ( 45.0%)\n",
      "  1 label(s): 21,254 samples ( 40.8%)\n",
      "  2 label(s):  6,064 samples ( 11.6%)\n",
      "  3 label(s):  1,266 samples (  2.4%)\n",
      "  4 label(s):     92 samples (  0.2%)\n",
      "  5 label(s):      1 samples (  0.0%)\n",
      "\n",
      "Training Set - Per-category distribution:\n",
      "  antitrust           :     69 (  0.1%)\n",
      "  civil_rights        :  2,690 (  5.2%)\n",
      "  crime               :  5,937 ( 11.4%)\n",
      "  govt_regulation     :  4,495 (  8.6%)\n",
      "  labor_movement      :  3,633 (  7.0%)\n",
      "  politics            : 19,242 ( 36.9%)\n",
      "  protests            :  1,487 (  2.9%)\n",
      "\n",
      "Test Set - Labels per sample:\n",
      "  0 label(s):  5,864 samples ( 45.0%)\n",
      "  1 label(s):  5,331 samples ( 40.9%)\n",
      "  2 label(s):  1,532 samples ( 11.7%)\n",
      "  3 label(s):    285 samples (  2.2%)\n",
      "  4 label(s):     27 samples (  0.2%)\n",
      "\n",
      "Test Set - Per-category distribution:\n",
      "  antitrust           :     24 (  0.2%)\n",
      "  civil_rights        :    703 (  5.4%)\n",
      "  crime               :  1,527 ( 11.7%)\n",
      "  govt_regulation     :  1,095 (  8.4%)\n",
      "  labor_movement      :    900 (  6.9%)\n",
      "  politics            :  4,763 ( 36.5%)\n",
      "  protests            :    346 (  2.7%)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, \n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"✓ Data split complete!\")\n",
    "print(f\"\\n  Training set:   {len(X_train):6,} samples ({len(X_train)/len(texts)*100:.1f}%)\")\n",
    "print(f\"  Test set:       {len(X_test):6,} samples ({len(X_test)/len(texts)*100:.1f}%)\")\n",
    "print(f\"  {'─'*50}\")\n",
    "print(f\"  Total:          {len(texts):6,} samples\")\n",
    "print(f\"\\n  Note: Validation set (20% of train) will be created during training\")\n",
    "\n",
    "# Show label distribution for both sets\n",
    "def show_distribution(y, name):\n",
    "    labels_per_sample = y.sum(axis=1)\n",
    "    print(f\"\\n{name} Set - Labels per sample:\")\n",
    "    unique_counts, counts = np.unique(labels_per_sample, return_counts=True)\n",
    "    for n_labels, count in zip(unique_counts, counts):\n",
    "        pct = (count / len(y)) * 100\n",
    "        print(f\"  {int(n_labels)} label(s): {count:6,} samples ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Show per-category statistics\n",
    "    print(f\"\\n{name} Set - Per-category distribution:\")\n",
    "    for i, category in enumerate(CATEGORIES):\n",
    "        count = int(y[:, i].sum())\n",
    "        pct = (count / len(y)) * 100\n",
    "        print(f\"  {category:20s}: {count:6,} ({pct:5.1f}%)\")\n",
    "\n",
    "show_distribution(y_train, \"Training\")\n",
    "show_distribution(y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "se2w8zhybpr",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data\n",
    "\n",
    "Save the datasets with one-hot encoded labels to parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data saved successfully!\n",
      "\n",
      "  ../data/train_data.parquet (52,152 samples)\n",
      "  ../data/test_data.parquet  (13,039 samples)\n",
      "\n",
      "======================================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Dataset structure:\n",
      "  - train_data.parquet: Training data (80%)\n",
      "  - test_data.parquet:  Test data (20%)\n",
      "  - Validation split: Created automatically during training (20% of train)\n",
      "\n",
      "Label format:\n",
      "  - One-hot encoded 7-dimensional numpy arrays\n",
      "  - [antitrust, civil_rights, crime, govt_regulation, labor_movement, politics, protests]\n",
      "  - Example: [0, 0, 1, 1, 0, 1, 0] = crime + govt_regulation + politics\n",
      "\n",
      "Next steps:\n",
      "  1. Config is set: NUM_CLASSES=7, VAL_SPLIT=0.2\n",
      "  2. Run training: python src/train.py\n",
      "  3. Model uses BCEWithLogitsLoss for multi-label classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "# Create DataFrames with one-hot encoded labels\n",
    "train_data = pd.DataFrame({\n",
    "    'text': X_train,\n",
    "    'label': list(y_train)  # Convert arrays to list for parquet storage\n",
    "})\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'label': list(y_test)\n",
    "})\n",
    "\n",
    "# Save to parquet format\n",
    "train_data.to_parquet(\"../data/train_data.parquet\", index=False)\n",
    "test_data.to_parquet(\"../data/test_data.parquet\", index=False)\n",
    "\n",
    "print(\"✓ Data saved successfully!\")\n",
    "print(f\"\\n  ../data/train_data.parquet ({len(train_data):,} samples)\")\n",
    "print(f\"  ../data/test_data.parquet  ({len(test_data):,} samples)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDataset structure:\")\n",
    "print(\"  - train_data.parquet: Training data (80%)\")\n",
    "print(\"  - test_data.parquet:  Test data (20%)\")\n",
    "print(\"  - Validation split: Created automatically during training (20% of train)\")\n",
    "print(\"\\nLabel format:\")\n",
    "print(\"  - One-hot encoded 7-dimensional numpy arrays\")\n",
    "print(\"  - [antitrust, civil_rights, crime, govt_regulation, labor_movement, politics, protests]\")\n",
    "print(\"  - Example: [0, 0, 1, 1, 0, 1, 0] = crime + govt_regulation + politics\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Config is set: NUM_CLASSES=7, VAL_SPLIT=0.2\")\n",
    "print(\"  2. Run training: python src/train.py\")\n",
    "print(\"  3. Model uses BCEWithLogitsLoss for multi-label classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
